{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\deshp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\deshp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\deshp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\deshp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\deshp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in c:\\users\\deshp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\deshp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\deshp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\deshp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\deshp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\deshp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\deshp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\deshp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\deshp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deshp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\deshp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deshp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n",
    "text = 'HEY THERE BABE MY NAME JEFF'\n",
    "\n",
    "encoded = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import MultiheadAttention, TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextEncoder, self).__init__() \n",
    "\n",
    "        self.encoder = TransformerEncoder(\n",
    "            encoder_layer=TransformerEncoderLayer(d_model=512, nhead=8),\n",
    "            num_layers=6\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(512, 512)\n",
    "    \n",
    "    def forward(self):    \n",
    "        return self.linear(self.encoder(encoded))\n",
    "    \n",
    "\n",
    "model = TextEncoder\n",
    "\n",
    "torch.save(model.state_dict, './transformerenc.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40814084, -0.37332957,  0.41736496,  0.56456025, -0.26873265,\n",
       "        -0.01499254,  1.53621261, -1.10453972],\n",
       "       [-0.03733441, -1.71067316, -0.06243443,  0.19111577,  0.2256944 ,\n",
       "        -0.75196169, -0.22457096, -0.96500822],\n",
       "       [-0.35982965,  0.12960284,  1.54211511, -1.08846447,  1.0357191 ,\n",
       "        -0.77953158,  0.27429107,  0.16560335],\n",
       "       [ 1.31532135,  0.05855913, -0.66361969, -0.47063576, -0.84205372,\n",
       "         0.0708315 , -0.93978473,  0.93058742]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "L, d_k, d_v = 4,8,8\n",
    "q = np.random.randn(L, d_k) # random number from the normal distribution\n",
    "v = np.random.randn(L, d_v)\n",
    "k = np.random.randn(L, d_k)\n",
    "\n",
    "q\n",
    "v\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02062012, -0.05819827,  0.91341086, -0.30045746],\n",
       "       [ 1.34369878, -0.49830369, -0.95924016, -0.48568279],\n",
       "       [ 1.08953297,  1.2021132 , -1.08492425, -0.17009201],\n",
       "       [-0.58208582,  0.47509466,  0.06232977,  0.15541014]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self-attention = ( scaled + masked ) softmax\n",
    "\n",
    "# scaled \n",
    "scaled = np.matmul(q, k.T) / np.sqrt(d_k)\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masked, its for the decoder\n",
    "\n",
    "# np.tril creates a triangular vector space\n",
    "mask = np.tril(np.ones( (L,L) ))\n",
    "mask[mask == 0] = -np.inf\n",
    "mask[mask == 1] = 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02062012,        -inf,        -inf,        -inf],\n",
       "       [ 1.34369878, -0.49830369,        -inf,        -inf],\n",
       "       [ 1.08953297,  1.2021132 , -1.08492425,        -inf],\n",
       "       [-0.58208582,  0.47509466,  0.06232977,  0.15541014]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask  # -inf such that no context needs to be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.86318536, 0.44786176, 0.12700329],\n",
       "       [0.        , 0.13681464, 0.50122987, 0.3655465 ],\n",
       "       [0.        , 0.        , 0.05090837, 0.24192521],\n",
       "       [0.        , 0.        , 0.        , 0.265525  ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax function\n",
    "# converts a vector into a probability dist, such that all row values add up to 1.\n",
    "\n",
    "def softmax(x):\n",
    "    return (np.exp(x).T)/np.sum(np.exp(x), axis=-1).T\n",
    "softmax(scaled+mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41170212 -0.73139916  0.39767492  1.15536245  0.80348307  0.67167837\n",
      "   1.57912703 -3.20815522]\n",
      " [ 0.41225352 -0.21620246  0.89882512  0.55613937  0.32930705  0.86711631\n",
      "   0.13231114 -0.48185702]\n",
      " [-0.10033232 -0.25774937  0.54043901 -0.06270601  0.00987469  0.57924564\n",
      "  -0.11411772 -0.11104061]\n",
      " [-0.1679168  -0.30765231  0.57179873 -0.13739818  0.00828832  0.60921472\n",
      "  -0.14761985 -0.11644557]]  new vector\n",
      "  \n",
      "[[-0.7595624  -0.68953117  0.45757243 -0.14960064 -1.08227542  1.49019217\n",
      "   0.61954042 -1.38742086]\n",
      " [ 0.91325518 -0.10794262 -0.58458619  0.95114877  2.15638205 -1.53224216\n",
      "   0.98575391 -1.99439988]\n",
      " [ 1.03440921  0.44312717  0.38228674  1.22730806  0.04563167  0.47492699\n",
      "   0.4003609  -0.09712925]\n",
      " [-0.63239547 -1.15865668  2.15346481 -0.51745856  0.03121482  2.29437804\n",
      "  -0.55595463 -0.43854842]]  original vector\n"
     ]
    }
   ],
   "source": [
    "new_vector = np.matmul(softmax(scaled+mask), v)\n",
    "\n",
    "print(new_vector, \" new vector\")\n",
    "print(\"  \")\n",
    "print(v, \" original vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as sp \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[512]' is invalid for input of size 2560",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\deshp\\Desktop\\😈\\testing_notebooks\\txtencoding.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/deshp/Desktop/%F0%9F%98%88/testing_notebooks/txtencoding.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn((batch_size, sequence_length, input_dim)) \u001b[39m# random data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/deshp/Desktop/%F0%9F%98%88/testing_notebooks/txtencoding.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/deshp/Desktop/%F0%9F%98%88/testing_notebooks/txtencoding.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mview(x\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/deshp/Desktop/%F0%9F%98%88/testing_notebooks/txtencoding.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/deshp/Desktop/%F0%9F%98%88/testing_notebooks/txtencoding.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m qkv_layer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(input_dim, \u001b[39m3\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[512]' is invalid for input of size 2560"
     ]
    }
   ],
   "source": [
    "##########################################################################################################################\n",
    "#######################################################################################################################\n",
    "# Multihead attention\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "sequence_length = 5\n",
    "batch_size = 1\n",
    "input_dim = 512\n",
    "d_model = 512\n",
    "x = torch.randn((batch_size, sequence_length, input_dim)) # random data\n",
    "print(x.shape)\n",
    "\n",
    "x = x.view(x.size(-1))\n",
    "print(x.shape)\n",
    "\n",
    "qkv_layer = nn.Linear(input_dim, 3)\n",
    "q,k,v = qkv_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\deshp\\Desktop\\😈\\testing_notebooks\\txtencoding.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/deshp/Desktop/%F0%9F%98%88/testing_notebooks/txtencoding.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m qkv \u001b[39m=\u001b[39m qkv\u001b[39m.\u001b[39mreshape(batch_size, sequence_length, num_heads, \u001b[39m3\u001b[39m\u001b[39m*\u001b[39mhead_dim)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/deshp/Desktop/%F0%9F%98%88/testing_notebooks/txtencoding.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m qkv\u001b[39m.\u001b[39mshape\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/deshp/Desktop/%F0%9F%98%88/testing_notebooks/txtencoding.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m q,k,v \u001b[39m=\u001b[39m qkv\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/deshp/Desktop/%F0%9F%98%88/testing_notebooks/txtencoding.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m q\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/deshp/Desktop/%F0%9F%98%88/testing_notebooks/txtencoding.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m k\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "num_heads = 8\n",
    "head_dim = d_model // num_heads\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3*head_dim)\n",
    "qkv.shape\n",
    "q,k,v = qkv\n",
    "q\n",
    "k\n",
    "v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
